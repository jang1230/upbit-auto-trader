"""
Historical Data Fetcher
Upbit APIë¥¼ í†µí•œ ê³¼ê±° ìº”ë“¤ ë°ì´í„° ìˆ˜ì§‘

ì£¼ìš” ê¸°ëŠ¥:
- ê³¼ê±° 1ë¶„ë´‰ ìº”ë“¤ ë°ì´í„° ìˆ˜ì§‘
- ë°ì´í„° ìºì‹± (CSV íŒŒì¼)
- ë°ì´í„° ê²€ì¦ ë° ê²°ì¸¡ì¹˜ ì²˜ë¦¬
"""

import time
import logging
from typing import Optional, List
from datetime import datetime, timedelta
from pathlib import Path
import pandas as pd
import requests

logger = logging.getLogger(__name__)


class HistoricalDataFetcher:
    """
    ê³¼ê±° ìº”ë“¤ ë°ì´í„° ìˆ˜ì§‘ê¸°

    Upbit APIë¥¼ í†µí•´ ê³¼ê±° ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ìºì‹±í•©ë‹ˆë‹¤.
    """

    def __init__(self, cache_dir: Optional[Path] = None):
        """
        ì´ˆê¸°í™”

        Args:
            cache_dir: ìºì‹œ ë””ë ‰í† ë¦¬ (Noneì´ë©´ í”„ë¡œì íŠ¸ ë£¨íŠ¸/data)
        """
        if cache_dir is None:
            project_root = Path(__file__).parent.parent
            cache_dir = project_root / 'data' / 'historical'

        self.cache_dir = cache_dir
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # Upbit API ì—”ë“œí¬ì¸íŠ¸
        self.base_url = "https://api.upbit.com/v1"

        logger.info(f"Historical Data Fetcher ì´ˆê¸°í™”")
        logger.info(f"  ìºì‹œ ë””ë ‰í† ë¦¬: {self.cache_dir}")

    def fetch_candles(
        self,
        symbol: str,
        start_date: datetime,
        end_date: datetime,
        interval: str = 'minute1',
        use_cache: bool = True
    ) -> pd.DataFrame:
        """
        ìº”ë“¤ ë°ì´í„° ìˆ˜ì§‘

        Args:
            symbol: ì‹¬ë³¼ (ì˜ˆ: 'KRW-BTC')
            start_date: ì‹œì‘ ë‚ ì§œ
            end_date: ì¢…ë£Œ ë‚ ì§œ
            interval: ìº”ë“¤ ê°„ê²© ('minute1', 'minute3', 'minute5', 'day')
            use_cache: ìºì‹œ ì‚¬ìš© ì—¬ë¶€

        Returns:
            pd.DataFrame: ìº”ë“¤ ë°ì´í„°
                index: timestamp (datetime)
                columns: open, high, low, close, volume
        """
        logger.info(f"ğŸ“Š ìº”ë“¤ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
        logger.info(f"  ì‹¬ë³¼: {symbol}")
        logger.info(f"  ê¸°ê°„: {start_date} ~ {end_date}")
        logger.info(f"  ê°„ê²©: {interval}")

        # ìºì‹œ íŒŒì¼ ê²½ë¡œ
        cache_file = self._get_cache_path(symbol, start_date, end_date, interval)

        # ìºì‹œ í™•ì¸
        if use_cache and cache_file.exists():
            logger.info(f"  âœ… ìºì‹œì—ì„œ ë¡œë“œ: {cache_file.name}")
            df = pd.read_csv(cache_file, index_col=0, parse_dates=True)
            return df

        # APIì—ì„œ ë°ì´í„° ìˆ˜ì§‘
        logger.info(f"  ğŸŒ Upbit APIì—ì„œ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
        df = self._fetch_from_api(symbol, start_date, end_date, interval)

        # ìºì‹œ ì €ì¥
        if use_cache:
            df.to_csv(cache_file)
            logger.info(f"  ğŸ’¾ ìºì‹œ ì €ì¥: {cache_file.name}")

        logger.info(f"  âœ… ìˆ˜ì§‘ ì™„ë£Œ: {len(df):,}ê°œ ìº”ë“¤")
        return df

    def _fetch_from_api(
        self,
        symbol: str,
        start_date: datetime,
        end_date: datetime,
        interval: str
    ) -> pd.DataFrame:
        """
        Upbit APIì—ì„œ ë°ì´í„° ìˆ˜ì§‘

        Args:
            symbol: ì‹¬ë³¼
            start_date: ì‹œì‘ ë‚ ì§œ
            end_date: ì¢…ë£Œ ë‚ ì§œ
            interval: ìº”ë“¤ ê°„ê²©

        Returns:
            pd.DataFrame: ìº”ë“¤ ë°ì´í„°
        """
        all_candles = []
        
        # Upbit APIì˜ 'to' íŒŒë¼ë¯¸í„°ëŠ” UTC ì‹œê°„ì„ ì‚¬ìš©í•˜ë¯€ë¡œ KST â†’ UTC ë³€í™˜ (9ì‹œê°„ ì°¨ì´)
        current_end_utc = end_date - timedelta(hours=9)
        start_date_utc = start_date - timedelta(hours=9)

        # Upbit APIëŠ” ìµœëŒ€ 200ê°œì”©ë§Œ ì¡°íšŒ ê°€ëŠ¥
        while current_end_utc > start_date_utc:
            # API ìš”ì²­ (UTC ì‹œê°„ ì‚¬ìš©)
            candles = self._request_candles(symbol, current_end_utc, interval, count=200)

            if not candles:
                break

            all_candles.extend(candles)

            # ë‹¤ìŒ ìš”ì²­ì„ ìœ„í•œ ì‹œê°„ ê³„ì‚° (ì—­ìˆœ ì •ë ¬ í›„ candles[0]ì´ ê°€ì¥ ì˜¤ë˜ëœ ìº”ë“¤)
            # ì£¼ì˜: APIì˜ 'to' íŒŒë¼ë¯¸í„°ëŠ” UTC ì‹œê°„ì„ ì‚¬ìš©!
            oldest_candle = candles[0]
            newest_candle = candles[-1]
            oldest_time_kst = pd.to_datetime(oldest_candle['candle_date_time_kst'])
            newest_time_kst = pd.to_datetime(newest_candle['candle_date_time_kst'])
            oldest_time_utc = pd.to_datetime(oldest_candle['candle_date_time_utc'])

            # APIì˜ 'to'ëŠ” "ì´í•˜"ì´ë¯€ë¡œ, 1ë¶„ì„ ë¹¼ì„œ ì¤‘ë³µ ë°©ì§€ (ë¶„ë´‰ì€ 1ë¶„ ë‹¨ìœ„)
            current_end_utc = oldest_time_utc - timedelta(minutes=1)

            logger.info(f"    ìˆ˜ì§‘: {len(candles)}ê°œ (ì´ {len(all_candles)}ê°œ) | ë²”ìœ„: {oldest_time_kst} ~ {newest_time_kst}")

            # Rate Limit ë°©ì§€ (ì´ˆë‹¹ 10íšŒ ì œí•œ)
            time.sleep(0.1)

            # ì‹œì‘ ë‚ ì§œ ì´ì „ì´ë©´ ì¢…ë£Œ (UTC ê¸°ì¤€ ë¹„êµ)
            if current_end_utc <= start_date_utc:
                logger.info(f"    ì¢…ë£Œ: {current_end_utc} <= {start_date_utc}")
                break

        # DataFrame ë³€í™˜
        df = self._convert_to_dataframe(all_candles)

        # ê¸°ê°„ í•„í„°ë§
        df = df[(df.index >= start_date) & (df.index <= end_date)]

        return df

    def _request_candles(
        self,
        symbol: str,
        to_datetime: datetime,
        interval: str,
        count: int = 200
    ) -> List[dict]:
        """
        Upbit API ìº”ë“¤ ì¡°íšŒ ìš”ì²­

        Args:
            symbol: ì‹¬ë³¼
            to_datetime: ë§ˆì§€ë§‰ ìº”ë“¤ ì‹œê°
            interval: ìº”ë“¤ ê°„ê²©
            count: ì¡°íšŒ ê°œìˆ˜ (ìµœëŒ€ 200)

        Returns:
            List[dict]: ìº”ë“¤ ë¦¬ìŠ¤íŠ¸
        """
        # interval ë§¤í•‘
        interval_map = {
            'minute1': 'minutes/1',
            'minute3': 'minutes/3',
            'minute5': 'minutes/5',
            'minute10': 'minutes/10',
            'minute15': 'minutes/15',
            'minute30': 'minutes/30',
            'minute60': 'minutes/60',
            'minute240': 'minutes/240',
            'day': 'days',
            'week': 'weeks',
            'month': 'months'
        }

        interval_path = interval_map.get(interval, 'minutes/1')

        # API URL
        url = f"{self.base_url}/candles/{interval_path}"

        # íŒŒë¼ë¯¸í„° (ISO 8601 í˜•ì‹ìœ¼ë¡œ ì „ë‹¬, ë§ˆì´í¬ë¡œì´ˆ ì œê±°)
        to_param = to_datetime.replace(microsecond=0).isoformat()
        params = {
            'market': symbol,
            'to': to_param,
            'count': count
        }

        try:
            response = requests.get(url, params=params, timeout=10)
            response.raise_for_status()
            data = response.json()

            # Upbit APIëŠ” ìµœì‹ â†’ê³¼ê±° ìˆœìœ¼ë¡œ ë°˜í™˜í•˜ë¯€ë¡œ ì—­ìˆœ ì •ë ¬ (ê³¼ê±°â†’ìµœì‹ )
            if isinstance(data, list):
                return data[::-1]
            return data

        except requests.exceptions.RequestException as e:
            logger.error(f"API ìš”ì²­ ì‹¤íŒ¨: {e}")
            return []

    def _convert_to_dataframe(self, candles: List[dict]) -> pd.DataFrame:
        """
        API ì‘ë‹µì„ DataFrameìœ¼ë¡œ ë³€í™˜

        Args:
            candles: API ì‘ë‹µ ìº”ë“¤ ë¦¬ìŠ¤íŠ¸

        Returns:
            pd.DataFrame: ìº”ë“¤ ë°ì´í„°
        """
        if not candles:
            return pd.DataFrame()

        # ë°ì´í„° ì¶”ì¶œ
        data = []
        for candle in candles:
            data.append({
                'timestamp': pd.to_datetime(candle['candle_date_time_kst']),
                'open': candle['opening_price'],
                'high': candle['high_price'],
                'low': candle['low_price'],
                'close': candle['trade_price'],
                'volume': candle['candle_acc_trade_volume']
            })

        # DataFrame ìƒì„±
        df = pd.DataFrame(data)
        df = df.set_index('timestamp')
        df = df.sort_index()

        return df

    def _get_cache_path(
        self,
        symbol: str,
        start_date: datetime,
        end_date: datetime,
        interval: str
    ) -> Path:
        """
        ìºì‹œ íŒŒì¼ ê²½ë¡œ ìƒì„±

        Args:
            symbol: ì‹¬ë³¼
            start_date: ì‹œì‘ ë‚ ì§œ
            end_date: ì¢…ë£Œ ë‚ ì§œ
            interval: ìº”ë“¤ ê°„ê²©

        Returns:
            Path: ìºì‹œ íŒŒì¼ ê²½ë¡œ
        """
        # íŒŒì¼ëª… ìƒì„±
        start_str = start_date.strftime('%Y%m%d')
        end_str = end_date.strftime('%Y%m%d')

        filename = f"{symbol}_{interval}_{start_str}_{end_str}.csv"

        return self.cache_dir / filename

    def clear_cache(self, symbol: Optional[str] = None):
        """
        ìºì‹œ ì‚­ì œ

        Args:
            symbol: íŠ¹ì • ì‹¬ë³¼ë§Œ ì‚­ì œ (Noneì´ë©´ ì „ì²´ ì‚­ì œ)
        """
        if symbol:
            # íŠ¹ì • ì‹¬ë³¼ ìºì‹œë§Œ ì‚­ì œ
            pattern = f"{symbol}_*.csv"
            deleted = 0
            for file in self.cache_dir.glob(pattern):
                file.unlink()
                deleted += 1
            logger.info(f"ìºì‹œ ì‚­ì œ: {symbol} ({deleted}ê°œ íŒŒì¼)")
        else:
            # ì „ì²´ ìºì‹œ ì‚­ì œ
            deleted = 0
            for file in self.cache_dir.glob("*.csv"):
                file.unlink()
                deleted += 1
            logger.info(f"ìºì‹œ ì „ì²´ ì‚­ì œ: {deleted}ê°œ íŒŒì¼")


# í…ŒìŠ¤íŠ¸ ì½”ë“œ
if __name__ == "__main__":
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    print("=" * 80)
    print("Historical Data Fetcher í…ŒìŠ¤íŠ¸")
    print("=" * 80)

    # ë°ì´í„° ìˆ˜ì§‘ê¸° ìƒì„±
    fetcher = HistoricalDataFetcher()

    # í…ŒìŠ¤íŠ¸: ìµœê·¼ 7ì¼ BTC 1ë¶„ë´‰ ë°ì´í„°
    end_date = datetime.now()
    start_date = end_date - timedelta(days=7)

    print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸: BTC 1ë¶„ë´‰ ë°ì´í„° ìˆ˜ì§‘")
    print(f"  ê¸°ê°„: {start_date} ~ {end_date}")

    df = fetcher.fetch_candles(
        symbol='KRW-BTC',
        start_date=start_date,
        end_date=end_date,
        interval='minute1',
        use_cache=True
    )

    print(f"\nâœ… ìˆ˜ì§‘ ì™„ë£Œ: {len(df):,}ê°œ ìº”ë“¤")
    print(f"\në°ì´í„° ë¯¸ë¦¬ë³´ê¸°:")
    print(df.head())
    print(f"\në°ì´í„° í†µê³„:")
    print(df.describe())

    print("\n" + "=" * 80)
